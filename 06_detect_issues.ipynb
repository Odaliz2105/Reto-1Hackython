{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0cd7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues detectados guardados en processed/detected_issues.json\n"
     ]
    }
   ],
   "source": [
    "# scripts/detect_issues.py\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "INPUT_JSON = \"processed/extracted_info.json\"\n",
    "OUTPUT_ISSUES = \"processed/detected_issues.json\"\n",
    "\n",
    "# Cargar extracción\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Secciones obligatorias que queremos asegurar\n",
    "REQUIRED_SECTIONS = [\n",
    "    \"clausulas_legales\",\n",
    "    \"requisitos_tecnicos\",\n",
    "    \"condiciones_economicas\",\n",
    "    # puedes añadir: \"garantias\", \"plazos\", \"multas\", etc.\n",
    "]\n",
    "\n",
    "# Detectar vacíos simples\n",
    "vacios = {}\n",
    "for sec in REQUIRED_SECTIONS:\n",
    "    val = data.get(sec, \"\")\n",
    "    if not val or (isinstance(val, str) and val.strip() == \"\"):\n",
    "        vacios[sec] = \"Falta o está vacío\"\n",
    "\n",
    "# Usa LLM para detectar ambigüedades o contradicciones (opcional)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = f\"\"\"\n",
    "Eres un auditor de contratos. Revisa el siguiente contenido (puede estar fragmentado) y\n",
    "devuelve un JSON con:\n",
    "- ambiguedades: lista de frases o cláusulas que son ambiguas o confusas.\n",
    "- contradicciones: lista de campos que se contradicen entre sí (por ejemplo, 'no hay multa' y 'multas: sí').\n",
    "Texto: {json.dumps(data, ensure_ascii=False)}\n",
    "\"\"\"\n",
    "\n",
    "resp = llm.invoke(prompt).content  # depende de tu versión; ajusta si usas LLMChain\n",
    "# Limpieza simple: intentar parsear JSON si devuelve; si no, guardar como texto.\n",
    "issues = {\"vacios\": vacios, \"llm_output\": resp}\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_ISSUES), exist_ok=True)\n",
    "with open(OUTPUT_ISSUES, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(issues, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Issues detectados guardados en\", OUTPUT_ISSUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010ae7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
